{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BloomTokenizerFast\n",
    "\n",
    "\n",
    "checkpoint = \"test_model\"#\"yuanzhoulvpi/chinese_bloom_7b_chat\"#\"bigscience/bloomz-3b\" #\"bigscience/bloom-7b1\"#  \"output_dir/checkpoint-8260\"#\n",
    "\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(\"D:/dddd/bloom_560m\")\n",
    "model_new = AutoModelForCausalLM.from_pretrained(\"test_model\").half().cuda()\n",
    "model_raw = AutoModelForCausalLM.from_pretrained(\"D:/dddd/bloom_560m\").half().cuda()\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "from typing import Optional\n",
    "def generate_input(instruction:Optional[str]= None, input_str:Optional[str] = None) -> str:\n",
    "    if input_str is None:\n",
    "        return PROMPT_DICT['prompt_no_input'].format_map({'instruction':instruction})\n",
    "    else:\n",
    "        return PROMPT_DICT['prompt_input'].format_map({'instruction':instruction, 'input':input_str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old: tensor([[21900, 15352]])\n",
      "new: tensor([[8421, 6425]])\n"
     ]
    }
   ],
   "source": [
    "with open(\"map_index.json\", mode='r', encoding='utf-8') as fin:\n",
    "    import json\n",
    "    oi2ni = json.loads(fin.read())\n",
    "oi2ni = {int(k):v for k,v in oi2ni.items()}\n",
    "oi2ni\n",
    "text_inputs = \"你是谁\"\n",
    "inputs = tokenizer.encode(text_inputs, return_tensors=\"pt\")\n",
    "inputs2 = tokenizer.encode(text_inputs, return_tensors=\"pt\")\n",
    "print(f\"old: {inputs}\")\n",
    "\n",
    "import torch\n",
    "\n",
    "def convertid(x):\n",
    "\n",
    "    if x < min(oi2ni.keys()):\n",
    "        return x \n",
    "    else:\n",
    "        return oi2ni.get(x, 0)\n",
    "\n",
    "\n",
    "inputs.apply_(lambda x: convertid(x))\n",
    "print(f\"new: {inputs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35187, 71479,  8433, 87152,  7561,     2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def convertid_rev(x):\n",
    "#     if x < min(oi2ni.item)\n",
    "\n",
    "oi2ni_rev = {v:k for k, v in oi2ni.items()}\n",
    "\n",
    "def convertid_rev(x):\n",
    "    if x < min(oi2ni.keys()):\n",
    "        return x \n",
    "    else:\n",
    "        return oi2ni_rev.get(x,0)\n",
    "\n",
    "\n",
    "test2 = torch.LongTensor([12051, 20318,  3997, 23533,  3665,2])\n",
    "test2.apply_(convertid_rev)\n",
    "test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 52922])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new.forward(input_ids=inputs.to('cuda')).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 250880])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_raw.forward(input_ids=inputs2.to('cuda'),return_dict=True).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_raw.forward(input_ids=inputs2.to('cuda'),return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0098, -0.0048, -0.0109,  ..., -0.0407,  0.0098,  0.0213],\n",
       "         [ 0.0040, -0.0131,  0.0141,  ..., -0.0431, -0.0002, -0.0118],\n",
       "         [ 0.0078,  0.0246,  0.0038,  ..., -0.0433, -0.0064,  0.0135],\n",
       "         ...,\n",
       "         [ 0.0080,  0.0057, -0.0108,  ..., -0.0429, -0.0013, -0.0279],\n",
       "         [ 0.0209, -0.0093,  0.0015,  ..., -0.0422, -0.0132,  0.0131],\n",
       "         [ 0.0089, -0.0001,  0.0003,  ..., -0.0434, -0.0122, -0.0062]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>),\n",
       " tensor([[-0.0098, -0.0048, -0.0109,  ..., -0.0407,  0.0098,  0.0213],\n",
       "         [ 0.0040, -0.0131,  0.0141,  ..., -0.0431, -0.0002, -0.0118],\n",
       "         [ 0.0078,  0.0246,  0.0038,  ..., -0.0433, -0.0064,  0.0135],\n",
       "         ...,\n",
       "         [ 0.0080,  0.0057, -0.0108,  ..., -0.0429, -0.0013, -0.0279],\n",
       "         [ 0.0209, -0.0093,  0.0015,  ..., -0.0422, -0.0132,  0.0131],\n",
       "         [ 0.0089, -0.0001,  0.0003,  ..., -0.0434, -0.0122, -0.0062]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_raw.lm_head.weight[:10, :],model_new.lm_head.weight[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([250880, 1024]), torch.Size([52922, 1024]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_row.lm_head.weight.shape,model.lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "tensor([8421, 6425,    2], device='cuda:0')\n",
      "你是谁</s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(\"*\"*80)\n",
    "    outputs = model_new.generate(inputs.to('cuda'),num_beams=3,\n",
    "                            max_new_tokens=512,\n",
    "                            do_sample=False, \n",
    "                            top_k=10,\n",
    "                            penalty_alpha=0.6,\n",
    "                            temperature=0.8,\n",
    "                            repetition_penalty=1.2)\n",
    "    print(outputs[0])\n",
    "    a = outputs[0].to('cpu')\n",
    "    a.apply_(convertid_rev)\n",
    "    print(tokenizer.decode(a))\n",
    "\n",
    "# 12051, 20318,  3997, 23533,  3665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "tensor([21900, 15352,     2], device='cuda:0')\n",
      "你是谁</s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(\"*\"*80)\n",
    "    outputs = model_raw.generate(inputs2.to('cuda'),num_beams=3,\n",
    "                            max_new_tokens=512,\n",
    "                            do_sample=False, \n",
    "                            top_k=10,\n",
    "                            penalty_alpha=0.6,\n",
    "                            temperature=0.8,\n",
    "                            repetition_penalty=1.2)\n",
    "    print(outputs[0])\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=1024, out_features=52922, bias=False),\n",
       " torch.Size([52922, 1024]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head, model.lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=4, out_features=2, bias=False),\n",
       " torch.Size([2, 4]),\n",
       " Parameter containing:\n",
       " tensor([[-0.0460,  0.2576, -0.1373,  0.0154],\n",
       "         [ 0.2853, -0.2333,  0.4222, -0.2241]], requires_grad=True))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "r1 = nn.Linear(4, 2, bias=False)\n",
    "r1, r1.weight.shape , r1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=4, out_features=2, bias=False),\n",
       " torch.Size([2, 4]),\n",
       " Parameter containing:\n",
       " tensor([[-0.0460,  0.2576, -0.1373,  0.0154],\n",
       "         [ 0.2853, -0.2333,  0.4222, -0.2241]], requires_grad=True))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = nn.Linear(4,2, bias=False)\n",
    "\n",
    "r2.weight.data = r1.weight.data.clone()\n",
    "\n",
    "r2, r2.weight.shape, r2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "data1 = torch.randn(5,4)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(r1(data1), r2(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
